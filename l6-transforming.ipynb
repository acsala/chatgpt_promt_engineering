{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNZEeWthCw/hlYrGwHQ4vO1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup\n","Load the API key and relevant Python libaries.\n","\n","In this course, we've provided some code that loads the OpenAI API key for you."],"metadata":{"id":"j93XswuRhdKh"}},{"cell_type":"code","source":[" !pip install --upgrade openai\n"," !pip install python-dotenv\n","\n","import openai\n","import os\n","\n","from dotenv import load_dotenv, find_dotenv\n","_ = load_dotenv(find_dotenv())\n","\n","# openai.api_key  = os.getenv('OPENAI_API_KEY')\n","openai.api_key  = \"sk-\""],"metadata":{"id":"ChkHQhmZhcwz","executionInfo":{"status":"ok","timestamp":1682752577234,"user_tz":-120,"elapsed":22821,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"06c7ff6b-ef40-4c30-a91a-a23173f654f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openai\n","  Downloading openai-0.27.5-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.5 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting python-dotenv\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.0\n"]}]},{"cell_type":"markdown","source":["helper function\n","\n","Throughout this course, we will use OpenAI's gpt-3.5-turbo model and the chat completions endpoint. https://platform.openai.com/docs/guides/chat\n","\n","This helper function will make it easier to use prompts and look at the generated outputs:\n"],"metadata":{"id":"_y9THrTNhren"}},{"cell_type":"code","source":["def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    response = openai.ChatCompletion.create(\n","        model=model,\n","        messages=messages,\n","        temperature=0, # this is the degree of randomness of the model's output\n","    )\n","    return response.choices[0].message[\"content\"]"],"metadata":{"id":"Ong97CKXhwAl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Translation\n","\n","ChatGPT is trained with sources in many languages. This gives the model the ability to do translation. Here are some examples of how to use this capability."],"metadata":{"id":"nDszwm4Qph_A"}},{"cell_type":"code","source":["prompt = f\"\"\"\n","Translate the following English text to Spanish: \\ \n","```Hi, I would like to order a blender```\n","\"\"\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"id":"i2g2tygFpk3T","executionInfo":{"status":"ok","timestamp":1682752588604,"user_tz":-120,"elapsed":1019,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"360fff02-4f6a-49ad-d46c-d86cd832155b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hola, me gustaría ordenar una licuadora.\n"]}]},{"cell_type":"code","source":["prompt = f\"\"\"\n","Tell me which language this is: \n","```Combien coûte le lampadaire?```\n","\"\"\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GcG0sCo3y2J","executionInfo":{"status":"ok","timestamp":1682752623978,"user_tz":-120,"elapsed":574,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"cabe3d6b-f71c-473f-e148-38a59e161c7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is French.\n"]}]},{"cell_type":"code","source":["prompt = f\"\"\"\n","Translate the following  text to French and Spanish\n","and English pirate: \\\n","```I want to order a basketball```\n","\"\"\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuowUhBU31e6","executionInfo":{"status":"ok","timestamp":1682752632656,"user_tz":-120,"elapsed":1389,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"e9106fc2-3c1a-4b50-cb9d-6eb610dbe939"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["French pirate: ```Je veux commander un ballon de basket```\n","Spanish pirate: ```Quiero pedir una pelota de baloncesto```\n","English pirate: ```I want to order a basketball```\n"]}]},{"cell_type":"code","source":["prompt = f\"\"\"\n","Translate the following text to Spanish in both the \\\n","formal and informal forms: \n","'Would you like to order a pillow?'\n","\"\"\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0qdjFLg34ch","executionInfo":{"status":"ok","timestamp":1682752639589,"user_tz":-120,"elapsed":1200,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"a088484e-c16c-44d8-c672-153f287e3d98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Formal: ¿Le gustaría ordenar una almohada?\n","Informal: ¿Te gustaría ordenar una almohada?\n"]}]},{"cell_type":"markdown","source":["# Universal Translator\n","\n","Imagine you are in charge of IT at a large multinational e-commerce company. Users are messaging you with IT issues in all their native languages. Your staff is from all over the world and speaks only their native languages. You need a universal translator!"],"metadata":{"id":"FtrAIlqH3-SI"}},{"cell_type":"code","source":["user_messages = [\n","  \"La performance du système est plus lente que d'habitude.\",  # System performance is slower than normal         \n","  \"Mi monitor tiene píxeles que no se iluminan.\",              # My monitor has pixels that are not lighting\n","  \"Il mio mouse non funziona\",                                 # My mouse is not working\n","  \"Mój klawisz Ctrl jest zepsuty\",                             # My keyboard has a broken control key\n","  \"我的屏幕在闪烁\"                                               # My screen is flashing\n","] "],"metadata":{"id":"lX-y2yis4BVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for issue in user_messages:\n","    prompt = f\"Tell me what language this is: ```{issue}```\"\n","    lang = get_completion(prompt)\n","    print(f\"Original message ({lang}): {issue}\")\n","\n","    prompt = f\"\"\"\n","    Translate the following  text to English \\\n","    and Korean: ```{issue}```\n","    \"\"\"\n","    response = get_completion(prompt)\n","    print(response, \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"ivCpLLGF4Eti","executionInfo":{"status":"error","timestamp":1682752690683,"user_tz":-120,"elapsed":1958,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"cf8b877a-ebe6-4329-8a42-c70f53b66494"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original message (This is French.): La performance du système est plus lente que d'habitude.\n","English: The system performance is slower than usual.\n","Korean: 시스템 성능이 평소보다 느립니다. \n","\n","Original message (This is Spanish.): Mi monitor tiene píxeles que no se iluminan.\n"]},{"output_type":"error","ename":"RateLimitError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c154fe0a871f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mKorean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0missue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-8db33b03e43d>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             return (\n\u001b[0;32m--> 620\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    684\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             )\n","\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-k49iPiRxqmz6JuYYm6kAWvmG on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."]}]},{"cell_type":"markdown","source":["# Tone Transformation\n","\n","Writing can vary based on the intended audience. ChatGPT can produce different tones.\n"],"metadata":{"id":"cZsZxKjb4SZt"}},{"cell_type":"code","source":["prompt = f\"\"\"\n","Translate the following from slang to a business letter: \n","'Dude, This is Joe, check out this spec on this standing lamp.'\n","\"\"\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSSB2NX24VD9","executionInfo":{"status":"ok","timestamp":1682752759585,"user_tz":-120,"elapsed":1353,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"a036c06c-6d1e-4023-da45-6f699c897bb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dear Sir/Madam,\n","\n","I am writing to bring to your attention a standing lamp that I believe may be of interest to you. Please find attached the specifications for your review.\n","\n","Thank you for your time and consideration.\n","\n","Sincerely,\n","\n","Joe\n"]}]},{"cell_type":"markdown","source":["# Format Conversion\n","\n","ChatGPT can translate between formats. The prompt should describe the input and output formats.\n"],"metadata":{"id":"LFnLHRIM4aYm"}},{"cell_type":"code","source":["data_json = { \"resturant employees\" :[ \n","    {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"},\n","    {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"},\n","    {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"}\n","]}\n","\n","prompt = f\"\"\"\n","Translate the following python dictionary from JSON to an HTML \\\n","table with column headers and title: {data_json}\n","\"\"\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZIZ6Mmb4b4n","executionInfo":{"status":"ok","timestamp":1682752794420,"user_tz":-120,"elapsed":3923,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"ddb8b705-6733-4546-a6a8-6d9b0a09642b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<table>\n","  <caption>Restaurant Employees</caption>\n","  <thead>\n","    <tr>\n","      <th>Name</th>\n","      <th>Email</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>Shyam</td>\n","      <td>shyamjaiswal@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Bob</td>\n","      <td>bob32@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Jai</td>\n","      <td>jai87@gmail.com</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]}]},{"cell_type":"code","source":["from IPython.display import display, Markdown, Latex, HTML, JSON\n","display(HTML(response))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"Ey8UuXDY4eup","executionInfo":{"status":"ok","timestamp":1682752797571,"user_tz":-120,"elapsed":274,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"19965016-3836-4d91-c5db-86f6d7cc2a01"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table>\n","  <caption>Restaurant Employees</caption>\n","  <thead>\n","    <tr>\n","      <th>Name</th>\n","      <th>Email</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>Shyam</td>\n","      <td>shyamjaiswal@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Bob</td>\n","      <td>bob32@gmail.com</td>\n","    </tr>\n","    <tr>\n","      <td>Jai</td>\n","      <td>jai87@gmail.com</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Spellcheck/Grammar check.\n","\n","Here are some examples of common grammar and spelling problems and the LLM's response.\n","\n","To signal to the LLM that you want it to proofread your text, you instruct the model to 'proofread' or 'proofread and correct'.\n"],"metadata":{"id":"s565UFDb4hNa"}},{"cell_type":"code","source":["text = f\"\"\"\n","Got this for my daughter for her birthday cuz she keeps taking \\\n","mine from my room.  Yes, adults also like pandas too.  She takes \\\n","it everywhere with her, and it's super soft and cute.  One of the \\\n","ears is a bit lower than the other, and I don't think that was \\\n","designed to be asymmetrical. It's a bit small for what I paid for it \\\n","though. I think there might be other options that are bigger for \\\n","the same price.  It arrived a day earlier than expected, so I got \\\n","to play with it myself before I gave it to my daughter.\n","\"\"\"\n","prompt = f\"proofread and correct this review: ```{text}```\"\n","response = get_completion(prompt)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLB-jHov4oX8","executionInfo":{"status":"ok","timestamp":1682752970187,"user_tz":-120,"elapsed":3678,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"0e81b180-dee5-4ee1-a0eb-4c0e693182a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I got this for my daughter's birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. Additionally, it's a bit small for what I paid for it. I think there might be other options that are bigger for the same price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\n"]}]},{"cell_type":"code","source":["!pip install redlines\n","\n","from redlines import Redlines\n","\n","diff = Redlines(text,response)\n","display(Markdown(diff.output_markdown))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":134},"id":"wYHvdljd4ptl","executionInfo":{"status":"ok","timestamp":1682752976240,"user_tz":-120,"elapsed":4442,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"37edb2d9-6a58-47c3-f076-321ee8ae6440"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: redlines in /usr/local/lib/python3.10/dist-packages (0.2.2)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<span style=\"color:red;font-weight:700;text-decoration:line-through;\">Got </span><span style=\"color:red;font-weight:700;\">I got </span>this for my <span style=\"color:red;font-weight:700;text-decoration:line-through;\">daughter for her </span><span style=\"color:red;font-weight:700;\">daughter's </span>birthday <span style=\"color:red;font-weight:700;text-decoration:line-through;\">cuz </span><span style=\"color:red;font-weight:700;\">because </span>she keeps taking mine from my <span style=\"color:red;font-weight:700;text-decoration:line-through;\">room.  </span><span style=\"color:red;font-weight:700;\">room. </span>Yes, adults also like pandas <span style=\"color:red;font-weight:700;text-decoration:line-through;\">too.  </span><span style=\"color:red;font-weight:700;\">too. </span>She takes it everywhere with her, and it's super soft and <span style=\"color:red;font-weight:700;text-decoration:line-through;\">cute.  One </span><span style=\"color:red;font-weight:700;\">cute. However, one </span>of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. <span style=\"color:red;font-weight:700;text-decoration:line-through;\">It's </span><span style=\"color:red;font-weight:700;\">Additionally, it's </span>a bit small for what I paid for <span style=\"color:red;font-weight:700;text-decoration:line-through;\">it though. </span><span style=\"color:red;font-weight:700;\">it. </span>I think there might be other options that are bigger for the same <span style=\"color:red;font-weight:700;text-decoration:line-through;\">price.  It </span><span style=\"color:red;font-weight:700;\">price. On the positive side, it </span>arrived a day earlier than expected, so I got to play with it myself before I gave it to my <span style=\"color:red;font-weight:700;text-decoration:line-through;\">daughter.\n</span><span style=\"color:red;font-weight:700;\">daughter.</span>"},"metadata":{}}]},{"cell_type":"code","source":["prompt = f\"\"\"\n","proofread and correct this review. Make it more compelling. \n","Ensure it follows APA style guide and targets an advanced reader. \n","Output in markdown format.\n","Text: ```{text}```\n","\"\"\"\n","response = get_completion(prompt)\n","display(Markdown(response))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"nDMrWz6O4xn2","executionInfo":{"status":"ok","timestamp":1682752881007,"user_tz":-120,"elapsed":8175,"user":{"displayName":"Attila Csala","userId":"12190727824513313405"}},"outputId":"0f96a0e1-4b3f-403d-9ca2-d64cb218faaf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Title: A Soft and Cute Panda Plush Toy for All Ages\n\nIntroduction:\nAs a parent, finding the perfect gift for your child's birthday can be a daunting task. However, I stumbled upon a soft and cute panda plush toy that not only made my daughter happy but also brought joy to me as an adult. In this review, I will share my experience with this product and provide an honest assessment of its features.\n\nProduct Description:\nThe panda plush toy is made of high-quality materials that make it super soft and cuddly. Its cute design is perfect for children and adults alike, making it a versatile gift option. The toy is small enough to carry around, making it an ideal companion for your child on their adventures.\n\nPros:\nThe panda plush toy is incredibly soft and cute, making it an excellent gift for children and adults. Its small size makes it easy to carry around, and its design is perfect for snuggling. The toy arrived a day earlier than expected, which was a pleasant surprise.\n\nCons:\nOne of the ears is a bit lower than the other, which makes the toy asymmetrical. Additionally, the toy is a bit small for its price, and there might be other options that are bigger for the same price.\n\nConclusion:\nOverall, the panda plush toy is an excellent gift option for children and adults who love cute and cuddly toys. Despite its small size and asymmetrical design, the toy's softness and cuteness make up for its shortcomings. I highly recommend this product to anyone looking for a versatile and adorable gift option."},"metadata":{}}]},{"cell_type":"markdown","source":["# Notes on using the OpenAI API outside of this classroom¶\n","\n","To install the OpenAI Python library:\n","\n","!pip install openai\n","\n","The library needs to be configured with your account's secret key, which is available on the website.\n","\n","You can either set it as the OPENAI_API_KEY environment variable before using the library:\n","\n"," !export OPENAI_API_KEY='sk-...'\n","\n","Or, set openai.api_key to its value:\n","\n","import openai\n","openai.api_key = \"sk-...\"\n","\n","A note about the backslash\n","\n","    In the course, we are using a backslash \\ to make the text fit on the screen without inserting newline '\\n' characters.\n","    GPT-3 isn't really affected whether you insert newline characters or not. But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance.\n","\n"],"metadata":{"id":"8sgS3aOSmDud"}}]}